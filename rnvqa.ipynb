{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RN is a neural network module with a structure primed for relational reasoning. The design philosophy behind RNs is to constrain the functional form of a neural network so that it captures the core common properties of relational reasoning. In other words, the capacity to compute relations is baked into the RN architecture without needing to be learned, just as the capacity to reason about spatial, translation invariant properties is built-in to CNNs, and the capacity to reason about sequential dependencies is built into recurrent neural networks.\n",
    "\n",
    "In its simplest form the RN is a composite function:\n",
    "\n",
    "$RN(O) = f_{\\phi}(\\sum_{i,j}g_{\\theta}(o_i,o_j))$\n",
    "\n",
    "where the input is a set of “objects” $O = {o_1, o_2, ..., o_n}, o_i \\in R^m$ is the ith object, and $f_{\\phi}$ and $g_{\\theta}$ are functions with parameters $\\phi$ and $\\theta$, respectively. For our purposes, $f_{\\phi}$ and $g_{\\theta}$ are MLPs, and the parameters are learnable synaptic weights, making RNs end-to-end differentiable. We call the output of $g_{\\theta}$ a “relation”; therefore, the role of $g_{\\theta}$ is to infer the ways in which two objects are related, or if they are even related at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNs have three notable strengths: they learn to infer relations, they are data efficient, and they operate on a set of objects – a particularly general and versatile input format – in a manner that is order invariant.\n",
    "\n",
    "1. RNs learn to infer relations :\n",
    "    The functional form in equation above dictates that an RN should consider the potential relations between all object pairs. This implies that an RN is not necessarily privy to which object relations actually exist, nor to the actual meaning of any particular relation. Thus, RNs must learn to infer the existence and implications of object relations. In graph theory parlance, the input can be thought of as a complete and directed graph whose nodes are objects and whose edges denote the object pairs whose relations should be considered. Although we focus on this “all-to-all” version of the RN throughout this paper, this RN definition can be adjusted to consider only some object pairs. Similar to Interaction Networks, to which RNs are related, RNs can take as input a list of only those pairs that should be considered, if this information is available. This information could be explicit in the input data, or could perhaps be extracted by some upstream mechanism.\n",
    "\n",
    "2. RNs are data efficient :\n",
    "    RNs use a single function $g_{\\theta}$ to compute each relation. This can be thought of as a single function operating on a batch of object pairs, where each member of the batch is a particular object-object pair from the same object set. This mode of operation encourages greater generalization for computing relations, since $g_{\\theta}$ is encouraged not to over-fit to the features of any particular object pair. Consider how an MLP would learn the same function. An MLP would receive all objects from the object set simultaneously as its input. It must then learn and embed $n^2$ (where n is the number of objects) identical functions within its weight parameters to account for all possible object pairings. This quickly becomes intractable as the number of objects grows. Therefore, the cost of learning a relation function $n^2$ times using a single feedforward pass per sample, as in an MLP, is replaced by the cost of $n^2$ feedforward passes per object set (i.e., for each possible object pair in the set) and learning a relation function just once, as in an RN\n",
    "\n",
    "3. RNs operate on a set of objects :\n",
    "    The summation in Equation above ensures that the RN is invariant to the order of objects in the input. This invariance ensures that the RN’s input respects the property that sets are order invariant, and it ensures that the output is order invariant. Ultimately, this invariance ensures that the RN’s output contains information that is generally representative of the relations that exist in the object set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AaBIobwY--dl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ./data already exists\n",
      "building test datasets...\n",
      "building train datasets...\n",
      "saving datasets...\n",
      "datasets saved at ./data\\sort-of-clevr-original.pickle\n"
     ]
    }
   ],
   "source": [
    "!python sort_of_clevr_generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --model Original_RN --epochs 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f884bc89b28f39021ed13e92233ce889ee4d7d42b588071f5428a4a01903dd89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
